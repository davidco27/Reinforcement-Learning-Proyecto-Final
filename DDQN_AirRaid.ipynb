{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310cc18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc51fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2a35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########  Using gym\n",
    "import gymnasium as gym\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "N_episodios = 250\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build the DQN. \n",
    "# Given a state, it will estimate, for each possible action, the sum of discounted future rewards it can expect after \n",
    "# it plays that action (but before it sees its outcome):\n",
    "class DDQN():\n",
    "    def __init__(self,batch_size= 32, discount_rate=0.5,loss_fn = keras.losses.mean_squared_error,learning_rate=1e-2):\n",
    "        self.best_score = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.discount_rate = discount_rate\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "\n",
    "        self.model = keras.models.Sequential([\n",
    "        keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "        keras.layers.Dense(64, activation=\"elu\"),\n",
    "        keras.layers.Dense(32, activation=\"elu\"),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(n_outputs)\n",
    "    ])\n",
    "\n",
    "        self.target = keras.models.clone_model(self.model)  # NEW It is a clone of the online model\n",
    "        self.target.set_weights(self.model.get_weights())   # NEW\n",
    "        \n",
    "    def epsilon_greedy_policy(self,state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(n_outputs)\n",
    "        else:\n",
    "            Q_values = self.model.predict(state[np.newaxis])\n",
    "            #print(\"Q_values\", Q_values)\n",
    "            return np.argmax(Q_values)\n",
    "        \n",
    "    def play_one_step(self,env, state, epsilon):\n",
    "        action = self.epsilon_greedy_policy(state, epsilon)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        replay_memory.append((state, action, reward, next_state, done))\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def sample_experiences(self,batch_size):\n",
    "        indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "        batch = [replay_memory[index] for index in indices]\n",
    "        states, actions, rewards, next_states, dones = [\n",
    "            np.array([experience[field_index] for experience in batch])\n",
    "            for field_index in range(5)]\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def training_step(self,batch_size):\n",
    "        \n",
    "        experiences = self.sample_experiences(batch_size)\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        #next_Q_values = model.predict(next_states)  # ORIGINAL DQN\n",
    "        next_Q_values = self.target.predict(next_states)  # NEW  Using the target model instead the online model \n",
    "        max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "        target_Q_values = (rewards +\n",
    "                        (1 - dones) * self.discount_rate * max_next_Q_values)\n",
    "        target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "        mask = tf.one_hot(actions, n_outputs)\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)   # Use of the online model\n",
    "            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    def predict(self,obs):\n",
    "        return self.model.predict(obs)\n",
    "    def train(self):\n",
    "        self.rewards = []\n",
    "        \n",
    "        for episode in range(N_episodios):\n",
    "            obs = env.reset() \n",
    "            obs = obs[0]\n",
    "            print(\"Se resetea\")\n",
    "            for step in range(200):\n",
    "                epsilon = max(1 - episode / 500, 0.01)\n",
    "                obs, reward, done, info = self.play_one_step(env, obs, epsilon)\n",
    "                if done:\n",
    "                    break\n",
    "            self.rewards.append(step) # Not shown in the book\n",
    "            if step >= self.best_score: # Not shown\n",
    "                best_weights = self.model.get_weights() # Not shown\n",
    "                self.best_score = step # Not shown\n",
    "            print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n",
    "            #print(\" Episodio vvvv\", episode)\n",
    "            if episode > 50:\n",
    "                #print(\" batch size vvvv\", batch_size)      \n",
    "                self.training_step(self.batch_size)\n",
    "            if episode % 50 == 0:     ## Updating weights\n",
    "                self.target.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.model.set_weights(best_weights)\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(),'logsDDQN')\n",
    "\n",
    "env = gym.make(\"ALE/AirRaid-v5\",render_mode=\"rgb_array\")\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "input_shape =env.observation_space.shape\n",
    "n_outputs = env.action_space.n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fad723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c513a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define los rangos para los parámetros que quieres optimizar\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    discount_rate = trial.suggest_uniform('discount_rate', 0.1, 0.90)\n",
    "\n",
    "    ddqn =DDQN(batch_size=batch_size,discount_rate=discount_rate,learning_rate=learning_rate)\n",
    "    ddqn.train()\n",
    "    # Define el callback para guardar el mejor modelo\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "    return ddqn.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54887c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 17:20:28,773] A new study created in memory with name: no-name-817d3fcc-d142-4b12-8d52-f4bb7fd60c6f\n",
      "C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_3972\\287002457.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
      "C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_3972\\287002457.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  discount_rate = trial.suggest_uniform('discount_rate', 0.1, 0.90)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se resetea\n",
      "Episode: 0, Steps: 200, eps: 1.000Se resetea\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "Episode: 1, Steps: 200, eps: 0.998Se resetea\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Episode: 2, Steps: 200, eps: 0.996Se resetea\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Episode: 3, Steps: 200, eps: 0.994Se resetea\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Episode: 4, Steps: 200, eps: 0.992Se resetea\n",
      "1/1 [==============================] - 0s 76ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: \", trial.value)\n",
    "\n",
    "print(\"Params: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f5f88",
   "metadata": {},
   "source": [
    "Con el mejor intento, lo entreno y saco los rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d1d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se resetea\n",
      "Episode: 0, Steps: 200, eps: 1.000Se resetea\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_30/dense_212/Tensordot/MatMul' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_8148\\1685122608.py\", line 2, in <module>\n      ddqn.train()\n    File \"C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_8148\\3616327690.py\", line 78, in train\n      obs, reward, done, info = self.play_one_step(env, obs, epsilon)\n    File \"C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_8148\\3616327690.py\", line 37, in play_one_step\n      action = self.epsilon_greedy_policy(state, epsilon)\n    File \"C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_8148\\3616327690.py\", line 32, in epsilon_greedy_policy\n      Q_values = self.model.predict(state[np.newaxis])\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2382, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\layers\\core\\dense.py\", line 244, in call\n      outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])\nNode: 'sequential_30/dense_212/Tensordot/MatMul'\nOOM when allocating tensor with shape[40000,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node sequential_30/dense_212/Tensordot/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_46583]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ddqn \u001b[38;5;241m=\u001b[39mDDQN(batch_size\u001b[38;5;241m=\u001b[39mtrial\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],discount_rate\u001b[38;5;241m=\u001b[39mtrial\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscount_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],learning_rate\u001b[38;5;241m=\u001b[39mtrial\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mddqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(ddqn\u001b[38;5;241m.\u001b[39mrewards)\n",
      "Cell \u001b[1;32mIn[25], line 78\u001b[0m, in \u001b[0;36mDDQN.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m     77\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m episode \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 37\u001b[0m, in \u001b[0;36mDDQN.play_one_step\u001b[1;34m(self, env, state, epsilon)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_one_step\u001b[39m(\u001b[38;5;28mself\u001b[39m,env, state, epsilon):\n\u001b[1;32m---> 37\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon_greedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     39\u001b[0m     replay_memory\u001b[38;5;241m.\u001b[39mappend((state, action, reward, next_state, done))\n",
      "Cell \u001b[1;32mIn[25], line 32\u001b[0m, in \u001b[0;36mDDQN.epsilon_greedy_policy\u001b[1;34m(self, state, epsilon)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(n_outputs)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     Q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m#print(\"Q_values\", Q_values)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(Q_values)\n",
      "File \u001b[1;32mc:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_30/dense_212/Tensordot/MatMul' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\raul\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_8148\\1685122608.py\", line 2, in <module>\n      ddqn.train()\n    File \"C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_8148\\3616327690.py\", line 78, in train\n      obs, reward, done, info = self.play_one_step(env, obs, epsilon)\n    File \"C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_8148\\3616327690.py\", line 37, in play_one_step\n      action = self.epsilon_greedy_policy(state, epsilon)\n    File \"C:\\Users\\raul\\AppData\\Local\\Temp\\ipykernel_8148\\3616327690.py\", line 32, in epsilon_greedy_policy\n      Q_values = self.model.predict(state[np.newaxis])\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2382, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\raul\\.conda\\envs\\noestructurados\\Lib\\site-packages\\keras\\layers\\core\\dense.py\", line 244, in call\n      outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])\nNode: 'sequential_30/dense_212/Tensordot/MatMul'\nOOM when allocating tensor with shape[40000,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node sequential_30/dense_212/Tensordot/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_46583]"
     ]
    }
   ],
   "source": [
    "ddqn =DDQN(batch_size=trial.params['batch_size'],discount_rate=trial.params['discount_rate'],learning_rate=trial.params['learning_rate'])\n",
    "ddqn.train()\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(ddqn.rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now show the animation:\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "env_id = \"ALE/AirRaid-v5\"\n",
    "directorio_video = os.path.join(os.getcwd(),'videosDDQN')\n",
    "\n",
    "longitud = 1819\n",
    " \n",
    "vec_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    " \n",
    "obs = vec_env.reset()\n",
    " \n",
    "# Record the video starting at the first step\n",
    "vec_env = VecVideoRecorder(vec_env, directorio_video,\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=longitud,\n",
    "                       name_prefix=f\"agent\")\n",
    "vec_env.reset()\n",
    "for _ in range(longitud + 1):\n",
    "  action, _states = ddqn.predict(obs)\n",
    "  obs, _, _, _ = vec_env.step(action)\n",
    "# Save the video\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831f9b9-f859-4071-97c8-dd7edbfb4de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
